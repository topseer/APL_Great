{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import patsy\n",
    "import sklearn.linear_model as linear\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Linear Models\n",
    "\n",
    "So far we have only looked at the simplest linear models that use only one feature--two if the other feature is a binary categorical feature (\"binary feature\", hereafter):\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x_1$\n",
    "\n",
    "The purpose of the simplification was to get you used to interpreting the coefficients of a linear model. There's nothing that prevents us from adding more features to the equation but as we add more features, the line becomes a *hyperplane* and a lot of intuition goes out the window. It's even difficult to chart these.\n",
    "\n",
    "The full linear model is:\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\dots + \\beta_n x_n + N(0, \\sigma)$\n",
    "\n",
    "Here we make a (nuanced) distinction between \"feature\" and \"variable\" and the proceed to ignore it because the nomenclature is too entrenched. A \"variable\" is from the data that you collect via ETL and explore during EDA. A feature is a variable put into a model and may be exactly the same as a variable, transformation, or both. For example, it could be the case that:\n",
    "\n",
    "$x_2 = mother\\_iq$\n",
    "\n",
    "and\n",
    "\n",
    "$x_3 = \\sqrt{mother\\_iq}$\n",
    "\n",
    "so the variable $mother\\_iq$ appears directly and indirectly as *two* features in the model.\n",
    "\n",
    "This means there may not be a one to one correspondence between variables in your database or other data source and the features in the model. Unfortunately, the term \"variable\" is so common in discussions that instead of fighting a losing battle, we will use \"variable\" and \"feature\" synonymously. Just remember, you often can and should transform your *variables* to create more or better *features*. We will discuss how later in this chapter.\n",
    "\n",
    "Now that we know that a regression can handle many features, how do we know which ones to add?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Regression Models\n",
    "\n",
    "The answer is probably not surprising, we should start with domain knowledge. The variables we have available are based on our discussion of the problem, meetings with domain experts, and qualitative modeling with Causal Loop Diagrams. We have wrestled to get the data (ETL) and wrestled with the data (EDA) and at this point we should have some intuitions about the variables, their relationships, and even possible transformations. This is a good starting point.\n",
    "\n",
    "As a result, our first pass at a model will likely include all the variables we believe are relevant to the target variable, given our knowledge of the domain and the data that we were able to get (rather than the data we *wanted* to get). Whether they were explicit or implicit, we have our Causal Loop Diagrams to guide us. By using domain knowledge, we also satisfy one of the underlying assumptions of correct modeling, the assumption of *validity*. The assumption of validity assumes all the variables have a reason for being in the model.\n",
    "\n",
    "Although there are techniques for the automatic selection of features, in general, selecting an optimal subset of features is NP-hard. It's easy to see why this is the case. If we have n variables and we want to select k features and build the best model we can, there are n choose k possibilities to pick from:\n",
    "\n",
    "$\\binom{n}{k} = \\frac{n!}{k(n-k)!}$\n",
    "\n",
    "for $n=10$ and $k=9$, that's 10 combinations. But what if the best set is 8? That's 45 more combinations. And if the best set is some 7 of them? 120 *more* combinations. That's a lot of combinations to try. And that doesn't include transformations or interaction terms. Increase the number of variables by 1 and you get even more possibilities.Domain knowledge at least gives us a foot in the door.\n",
    "\n",
    "We also have the problem that as the sizes of data sets increases, spurious correlations are more likely to result. We don't want to create and field a model that stops working because a relationship we were depending on was ephemeral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Variables? Why One Leg is better than Two\n",
    "\n",
    "Suppose we have data on human height and measurements for both the left and right leg. Here's a bit of Trivial Pursuit domain knowledge for you:\n",
    "\n",
    "1. leg length is typically 40-50% of total height which would make leg length a fairly good predictor of height.\n",
    "2. legs, however, are not always the same length. 70% of people typically have one leg longer than the other. The average difference is generally less than 1.1 cm. Differences of 0-0.30 cm is considered a \"mild\" difference, 0.31-0.60, is considered moderate and greater than 0.61 is considered severe.\n",
    "\n",
    "Let's model this system. We just need an average height in centimeters. The average Australian female is 161.8 cm tall. That's about 5' 3\" tall. Let's assume that heights are normally distributed with a coefficient of variation of about 10%. That gives us a range of heights. We'll pick an average leg length of 45% with 2.5% points as the standard deviation which will give us 95% of the data between 40-50%. We'll say that's the longer leg.\n",
    "\n",
    "The shorter leg is generally 1.1 cm shorter than the longer leg and even then 30% of the people don't have noticeably different leg lengths. This is a mixture model: a test to see if a leg length is shorter and then generating a difference. The difference is generally less than 1.1 cm. We can pick uniformly because it doesn't really matter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(47383484)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"height\"] = stats.norm.rvs(161.8, 16.2, 100)\n",
    "data[\"long_leg\"] = data['height'] * stats.norm.rvs(0.45, 0.025, 100)\n",
    "data = pd.DataFrame(data)\n",
    "data[\"short_leg\"] = data[\"long_leg\"].apply(lambda x: x if stats.uniform.rvs() < 0.3 else x - stats.uniform.rvs(0.0, 1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we've generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>long_leg</th>\n",
       "      <th>short_leg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>160.131744</td>\n",
       "      <td>72.152456</td>\n",
       "      <td>71.759224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.344076</td>\n",
       "      <td>8.446286</td>\n",
       "      <td>8.441024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>128.082617</td>\n",
       "      <td>55.541141</td>\n",
       "      <td>55.337981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>149.066740</td>\n",
       "      <td>65.251453</td>\n",
       "      <td>65.092133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>158.561970</td>\n",
       "      <td>71.523243</td>\n",
       "      <td>71.125217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>170.632393</td>\n",
       "      <td>77.945643</td>\n",
       "      <td>77.789251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>202.151122</td>\n",
       "      <td>98.053558</td>\n",
       "      <td>97.670528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           height    long_leg   short_leg\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean   160.131744   72.152456   71.759224\n",
       "std     15.344076    8.446286    8.441024\n",
       "min    128.082617   55.541141   55.337981\n",
       "25%    149.066740   65.251453   65.092133\n",
       "50%    158.561970   71.523243   71.125217\n",
       "75%    170.632393   77.945643   77.789251\n",
       "max    202.151122   98.053558   97.670528"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that looks good but we need to get rid of one artifact we created when generating the data. We don't generally collect data as \"short\" and \"long\" leg but \"left\" and \"right\" leg. Let's shuffle those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pick(x1, x2):\n",
    "    return x1 if stats.uniform.rvs() < 0.5 else x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"total_leg\"] = data[\"long_leg\"] + data[\"short_leg\"]\n",
    "data[\"left_leg\"] = data[[\"long_leg\", \"short_leg\"]].apply(lambda x: random_pick(*x), axis=1)\n",
    "data[\"right_leg\"] = data[\"total_leg\"] - data[\"left_leg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the code to generate the linear regression model for `height ~ left_leg + right_leg`, what do you expect the coefficients ($\\beta_1$ and $\\beta_2$) on each leg to be? It seems reasonable to think that they would both be positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../resources')\n",
    "import fundamentals.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <table>\n",
       "    <tr><th> model </th><td> height ~ left_leg + right_leg </td></tr>\n",
       "    <tr><th colspan=2>coefficients</th><th>95% BCI</th></tr>\n",
       "    <tr><th> $\\beta_0$ </th><td> 45.50 </td><td>(36.08, 57.50)</td></tr><tr><td>  left_leg  ($\\beta_1$) </td><td> 0.68 </td><td>(-2.69, 3.14)</tr><tr><td>  right_leg ($\\beta_2$) </td><td> 0.91 </td><td>(-1.59, 4.22)</tr><tr><th colspan=2>metrics</th><th>95% BCI</th></tr><tr><th> $\\sigma$ </th><td> 7.47 </td><td>(6.24, 8.48) </td></tr><tr><th> $R^2$ </th><td> 0.77 </td><td>(0.65, 0.84)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = models.bootstrap_linear_regression(\"height ~ left_leg + right_leg\", data)\n",
    "models.describe_bootstrap_lr(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ looks pretty good. Our model captures about 77% of the variability of height. And the coefficients seem to be positive. However, if we look at the credible intervals, there's a strong probability that either or both of the coefficients are negative as well. Based on the data, we don't know what sign the coefficients should be. That seems strange.\n",
    "\n",
    "This phenomenon goes by the name of *multicolinearity*. So we cannot simply include all of the variables we have even if domain knowledge suggests that they may all be relevant. One way to detect the possibility of such problems is to check for correlations between your features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979106583969305"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(data.left_leg, data.right_leg)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need only check Pearson's Correlation Coefficient because the correlation between the two variables must be linear. Note that checking all pairs of (numerical) variables lands us in the combinatorial explosion again but it's not quite as bad. We can use domain knowledge to prune the pairs we need to check. And we can also check coefficients to see if they have unexpected signs or low credibility for the expected sign.\n",
    "\n",
    "What should we do?\n",
    "\n",
    "A general approach is to:\n",
    "\n",
    "1. include the variable that correlates most highly with the target variable.\n",
    "2. construct a new variable that combines the information of the other, correlated variables by taking a sum, average, min, max, etc.\n",
    "\n",
    "Let's try an average leg length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"average_leg\"] = data[\"total_leg\"] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <table>\n",
       "    <tr><th> model </th><td> height ~ average_leg </td></tr>\n",
       "    <tr><th colspan=2>coefficients</th><th>95% BCI</th></tr>\n",
       "    <tr><th> $\\beta_0$ </th><td> 45.51 </td><td>(35.24, 59.74)</td></tr><tr><td>  average_leg ($\\beta_1$) </td><td> 1.59 </td><td>(1.39, 1.73)</tr><tr><th colspan=2>metrics</th><th>95% BCI</th></tr><tr><th> $\\sigma$ </th><td> 7.43 </td><td>(6.34, 8.39) </td></tr><tr><th> $R^2$ </th><td> 0.77 </td><td>(0.70, 0.85)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = models.bootstrap_linear_regression(\"height ~ average_leg\", data)\n",
    "models.describe_bootstrap_lr(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model is interpretable. Notice that the $R^2$ and $\\sigma$ are unaffected. Multicollinearity is very often a concern only if you are building an explanatory model and not simply a predictive one. It's worth remembering the exchange between Blomberg and Gelman from the Introduction:\n",
    "\n",
    "Simon Blomberg said,\n",
    "\n",
    "> machine learning is statistics minus any checking of models and assumptions -- Brian D. Ripley [Two Cultures](https://stats.stackexchange.com/questions/6/the-two-cultures- statistics-vs-machine-learning)\n",
    "\n",
    "To which Bayesian statistician Andrew Gelman responded,\n",
    "\n",
    "> In that case, maybe we should get rid of checking of models and assumptions more often. Then maybe we'd be able to solve some of the problems that the machine learning people can solve but we can't!\n",
    "\n",
    "There are a few things that matter greatly to the typical statistician that do not concern a machine learning engineer at all. For example, Andrew Ng teaches linear regression in his online Machine Learning course but he never once mentions multicollinearity. That doesn't mean we can ignore it, too. You have to know what the purpose of your model is. And even if it is only estimation, that doesn't mean somewhere down the road, a regulatory body isn't going to ask you to explain what it does. I present Gelman's advice throughout because I think it's a good middle ground between the two extremes. We concentrate on differences that *make* a difference when the difference is important.\n",
    "\n",
    "We will now modify our steps for building linear models:\n",
    "\n",
    "1. Start with all features as suggested by domain knowledge, but...\n",
    "2. Analyze features for correlations and, of those groups, consider picking the best variable, an average of the variables, or some other transformation (sum, min, max, etc).\n",
    "\n",
    "Think back to the EDA chapter and the King's County Housing Example. Do you think that sqft_living, sqft_above, sqft_below, sqft_living15 are all correlated? What about sqft_lot and sqft_lot15?\n",
    "\n",
    "So now we have an extra goal for EDA. If we know that we are going to be building a linear model, we should investigate correlations between variables that we plan to include in our linear model (and not just correlations between the target variables--price in this case--and the features). We add this now because if we're using a different kind of model, the problem of multicollinearity might not even arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Binary Features\n",
    "\n",
    "As we have just seen, although linear models are perfectly general, but there can be problems. If there is multicollinearity between two or more features, we will have some difficulty getting good estimates of their coefficients. The values of the coefficients are important if we wish to explain the relationship between the target variable and the features.\n",
    "\n",
    "Although it's more difficult to introduce multicollinearity between binary features, it's not impossible. Consider the canonical binary feature (that's not entirely binary): gender. Suppose we have $gender$ in the linear model below:\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 gender + \\beta_2 income$\n",
    "\n",
    "Our first observation is that when it comes to binary variables, we should name them whatever category is \"1\". So if it's 0 if female and 1 if male, we should just name the variable \"male\". This helps immeasurably with interpretation:\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 male + \\beta_2 income$\n",
    "\n",
    "With this model, $\\beta_1$ becomes the differential effect being male. Where's the female effect? It's in $\\beta_0$.\n",
    "\n",
    "But why can't we we have a variable, $female$ too and have $\\beta_0$ be zero?\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 male + \\beta_2 income + \\beta_3 female$\n",
    "\n",
    "Well, if we think about it a second, this would mean that the data looks like this:\n",
    "\n",
    "|  | male | female | income |\n",
    "|---|------|--------|--------|\n",
    "| 1 | 1 | 0 | 34000 |\n",
    "| 2 | 0 | 1 | 38000 |\n",
    "| 3 | 1 | 0 | 42000 |\n",
    "| 4 | 1 | 0 | 40000 |\n",
    "| 5 | 0 | 1 | 29000 |\n",
    "\n",
    "$male$ and $female$ are just the opposite of each other so there's no new information to be gained from including $female$ as a variable. And if male is 1 and female is 0, what is $\\beta_0$? This doesn't make sense. All of this seems obvious because we're used to \"natural\" binary variables like gender, purchased, voted, etc.\n",
    "\n",
    "What happens if we have a new variable, politics, that has three values: {left, center, right}. Our first thought might be to code these as {1, 2, 3} but that doesn't make sense for linear models: left + center = right? Um, no. Instead we can do a \"one hot encoding\": left {0, 1}, center: {0, 1}, right: {0, 1}. Now, here's the question. If we want to include these variables in the linear model, \n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 male + \\beta_2 income + \\beta_3 left + \\beta_4 center + \\beta_5 right$\n",
    "\n",
    "does it look like the above? No and for the same reason that we didn't include both male and female in the model. Here's the rule:\n",
    "\n",
    "> Every non-binary variable must be converted to a one hot encoding using the label as the feature name if possible. This will create $m$ new binary variables if the original variable has $m$ labels or outcomes. However, you can only include $m-1$ of these new features in your linear model. The missing variable gets pushed into the interpretation of $\\beta_0$, the intercept.\n",
    "\n",
    "Let's see how this works. We're going to take out \"left\":\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 male + \\beta_2 income + \\beta_3 center + \\beta_4 right$\n",
    "\n",
    "We can see that the male effect is $\\beta_1$, the center effect is $\\beta_3$, and the right effect is $\\beta_4$. Where is the female effect? $\\beta_0$. Where is the left effect? $\\beta_0$. In fact, $\\beta_0$ is now \"left females\".\n",
    "\n",
    "Where is the effect for right females? $\\beta_0 + \\beta_4$. Left males? $\\beta_0 + \\beta_1$.\n",
    "\n",
    "This makes a strong assumption that politics and gender are independent (no pun intended) and this may not be the case. We may want to create interaction terms. However,\n",
    "\n",
    "> the more specific the terms in your model, the more data you need to estimate it\n",
    "\n",
    "If you don't have a lot of Pagan, right, low income males in your data set...you're not going to get good estimates of the coefficients. Binary variables are always partitioning your data set into subpopulations. As we have seen, their presence in a model doesn't have the same effect on the model as a numeric feature.\n",
    "\n",
    "It follows that of the $m$ one hot encodings, which one do we push into $\\beta_0$? It is often said that it doesn't matter but I disagree. You need to take the relative frequency of the subpopulations into account.\n",
    "\n",
    "Consider what happens if you drop the binary variable with the fewest observations. With fewer observations, $\\beta_0$ will have a higher variance than the incremental effect $\\beta_1$ (for example). This might not be good if we need a solid estimate of the base effects.\n",
    "\n",
    "However, if we're interested *only* in the differential effect (how much does it change in the presence of the category), dropping the encoding out of $m$ with the fewest observations might be satisfactory. Conversely, if we *do* want good base rate estimates, then we want to drop the encoding out of $m$ with the most observations. Another option is to combine categories in meaningful ways to increase the number of observations they cover.\n",
    "\n",
    "No matter what, at least for primary effects, one of the one hot encodings for each such transformed categorical variable needs to be left out. The value will be pushed into the interpretation of $\\beta_0$.\n",
    "\n",
    "We will now modify our steps for building linear models:\n",
    "\n",
    "1. Start with all features as suggested by domain knowledge, but...\n",
    "2. Analyze features for correlations and, of those groups, consider picking the best variable, an average of the variables, or some other transformation (sum, min, max, etc).\n",
    "3. Transform all categorical variables into one hot encodings but leave one encoding out of the model for each variable. The intercept $\\beta_0$ represent all the outcomes that are excluded explicitly. Which one you leave out might depend on the number of observations for each and what you want to do with the model.\n",
    "\n",
    "Most data libraries have functions to create \"dummy variables\" (\"dummy\" in the sense of \"standing in for something real\" as in a \"crash test dummy\"). Pandas is no exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>politics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  politics\n",
       "0   center\n",
       "1   center\n",
       "2   center\n",
       "3   center\n",
       "4     left"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics = {\"politics\": np.random.choice([\"left\", \"center\", \"right\"], size=10)}\n",
    "data = pd.DataFrame(politics)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>politics</th>\n",
       "      <th>center</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>center</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>center</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>center</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>center</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>left</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  politics  center  left  right\n",
       "0   center       1     0      0\n",
       "1   center       1     0      0\n",
       "2   center       1     0      0\n",
       "3   center       1     0      0\n",
       "4     left       0     1      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, pd.get_dummies(data[\"politics\"])], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what? We estimate our model. For which we'll go back to Child IQs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Child IQs\n",
    "\n",
    "We've already seen this data but let's quickly review all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_iq = pd.read_csv( \"../resources/data/child_iq.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 434 entries, 0 to 433\n",
      "Data columns (total 5 columns):\n",
      "child_iq    434 non-null int64\n",
      "mom_hs      434 non-null int64\n",
      "mom_iq      434 non-null float64\n",
      "mom_work    434 non-null int64\n",
      "mom_age     434 non-null int64\n",
      "dtypes: float64(1), int64(4)\n",
      "memory usage: 17.0 KB\n"
     ]
    }
   ],
   "source": [
    "child_iq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable is child_iq. Our possible features are mom_hs, mom_iq, mom_work (did the mom work during the preschool years), and the mother's age, mom_age. We can guess that mom_iq and mom_age are numerical but what about the others? Let's see some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>child_iq</th>\n",
       "      <th>mom_hs</th>\n",
       "      <th>mom_iq</th>\n",
       "      <th>mom_work</th>\n",
       "      <th>mom_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>121.117529</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>89.361882</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>115.443165</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>99.449639</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>92.745710</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   child_iq  mom_hs      mom_iq  mom_work  mom_age\n",
       "0        65       1  121.117529         4       27\n",
       "1        98       1   89.361882         4       25\n",
       "2        85       1  115.443165         4       27\n",
       "3        83       1   99.449639         3       25\n",
       "4       115       1   92.745710         4       27"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_iq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both mom_hs and mom_work seem to encodings of some kind. Let's see how many values they have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    341\n",
       "0     93\n",
       "Name: mom_hs, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_iq.mom_hs.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already knew, mom_hs is a binary variable. What about mom_work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    205\n",
       "2     96\n",
       "1     77\n",
       "3     56\n",
       "Name: mom_work, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_iq.mom_work.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mom_work has four possible values. What are these values? According to the data dictionary:\n",
    "\n",
    "1. mother did not work for the first 3 years of the child's life.\n",
    "2. mother worked in second or third year of child's life.\n",
    "3. mother worked part-time during first year of child's life.\n",
    "4. mother worked full-time during first year of child's life.\n",
    "\n",
    "As we can see, the most common outcome is \"4\".\n",
    "\n",
    "We'll need to change this variable into a set of dummy variables. Because the labels are numbers, we have two options:\n",
    "\n",
    "1. change the values into labels in a new variable and then create the dummy variables.\n",
    "2. specify a prefix for the dummy values.\n",
    "\n",
    "For this variable, for the first option, we might have \"no_work\", \"some_work\", \"part_time\", and \"full_time\". For the second option, we can just use \"mom_worked\" as the prefix and it'll create dummies for \"mom_worked_1\", \"mom_worked_2\", \"mom_worked_3\" and \"mom_worked_4\". We're going to take the easy way out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_iq = pd.concat([child_iq, pd.get_dummies(child_iq[\"mom_work\"], prefix=\"mom_worked\")], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>child_iq</th>\n",
       "      <th>mom_hs</th>\n",
       "      <th>mom_iq</th>\n",
       "      <th>mom_work</th>\n",
       "      <th>mom_age</th>\n",
       "      <th>mom_worked_1</th>\n",
       "      <th>mom_worked_2</th>\n",
       "      <th>mom_worked_3</th>\n",
       "      <th>mom_worked_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>121.117529</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>89.361882</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>115.443165</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>99.449639</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>92.745710</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   child_iq  mom_hs      mom_iq  mom_work  mom_age  mom_worked_1  \\\n",
       "0        65       1  121.117529         4       27             0   \n",
       "1        98       1   89.361882         4       25             0   \n",
       "2        85       1  115.443165         4       27             0   \n",
       "3        83       1   99.449639         3       25             0   \n",
       "4       115       1   92.745710         4       27             0   \n",
       "\n",
       "   mom_worked_2  mom_worked_3  mom_worked_4  \n",
       "0             0             0             1  \n",
       "1             0             0             1  \n",
       "2             0             0             1  \n",
       "3             0             1             0  \n",
       "4             0             0             1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_iq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have mom_iq, mom_age, mom_hs, mom_worked_1 (no work), mom_work_2 (some work), mom_worked_3 (part time), and mom_worked_4 (full time). Before continuing, what do you think the *sign* of each coefficient ($\\beta_i$) is going to be?\n",
    "\n",
    "* `mom_hs`: positive. This could be a proxy for a number of things including income, demographics, intelligence outside general intelligence testing.\n",
    "* `mom_iq`: positive. It's likely there's a positive relationship between the mother's IQ and the child's.\n",
    "* `mom_age`: positive. Older mothers might be better educated or more experienced. This could be a proxy for other things including income, demographics, etc.\n",
    "* `mom_worked_1`: excluded - this the most common outcome for the mom_worked variable so we'll excluded it from explicit inclusion in the model.\n",
    "* `mom_worked_2`: negative - relative to the baseline, this should show a small decrease in IQ although overall this may be a signifier for demographics and income (if you can afford to take off the first year, you may have good maternity leave or support from the husband's income).\n",
    "* `mom_worked_3`: negative but more so than mom_worked_2 because it's part-time work throughout the 3 years.\n",
    "* `mom_worked_4`: negative but more so than mom_worked_3 because it's full-time work throughout the 3 years.\n",
    "\n",
    "It's interesting to think about the overall implications of the validity assumption here. Depending on the source of the data, this model might not be applicable to minorities, for example. The validity assumption is broader than *just* including relevant variables but also making relevant estimates that are not \"out of sample\".\n",
    "\n",
    "Now let's estimate the model: \"child_iq ~ mom_hs + mom_iq + mom_age + mom_worked_2 + mom_worked_3 + mom_worked_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <table>\n",
       "    <tr><th> model </th><td> child_iq ~ mom_hs + mom_iq + mom_age + mom_worked_2 + mom_worked_3 + mom_worked_4 </td></tr>\n",
       "    <tr><th colspan=2>coefficients</th><th>95% BCI</th></tr>\n",
       "    <tr><th> $\\beta_0$ </th><td> 20.27 </td><td>(-0.82, 39.32)</td></tr><tr><td>  mom_hs  ($\\beta_1$) </td><td> 5.43 </td><td>(0.83, 9.64)</tr><tr><td>  mom_iq  ($\\beta_2$) </td><td> 0.55 </td><td>(0.43, 0.67)</tr><tr><td>  mom_age  ($\\beta_3$) </td><td> 0.22 </td><td>(-0.57, 0.85)</tr><tr><td>  mom_worked_2  ($\\beta_4$) </td><td> 2.98 </td><td>(-2.23, 9.33)</tr><tr><td>  mom_worked_3  ($\\beta_5$) </td><td> 5.49 </td><td>(0.14, 10.18)</tr><tr><td>  mom_worked_4 ($\\beta_6$) </td><td> 1.42 </td><td>(-2.91, 5.66)</tr><tr><th colspan=2>metrics</th><th>95% BCI</th></tr><tr><th> $\\sigma$ </th><td> 18.14 </td><td>(17.06, 19.24) </td></tr><tr><th> $R^2$ </th><td> 0.22 </td><td>(0.15, 0.30)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"child_iq ~ mom_hs + mom_iq + mom_age + mom_worked_2 + mom_worked_3 + mom_worked_4\"\n",
    "result = models.bootstrap_linear_regression(model, data=child_iq)\n",
    "models.describe_bootstrap_lr(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Results\n",
    "\n",
    "First, we observe that the $R^2$ is 0.22 or 22%. The model explains 22% of the variation in child_iq. However, we note that the Coefficient of Determination is affected by the number of features in your model. That is, other things being equal, as you go from one feature to two to three to four, etc., the $R^2$ will not go down. It may stay the same or increase but it won't go down. That is, $R^2$ is a non-decreasing monotonic function of the number of variables. It is thus prudent to evaluate the *Adjusted* $R^2$ or $\\bar{R}^2$ which is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bar{R}^2 = 1 - (1-R^2) \\frac{n-1}{n-p-1}$\n",
    "\n",
    "where $n$ is the number of observations and $p$ is the number of features in the model. We can estimate ours here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_r_squared(result):\n",
    "    adjustment = (result[\"n\"] - 1)/(result[\"n\"] - len(result[\"coefficients\"]) - 1 - 1)\n",
    "    return 1 - (1 - result[\"r_squared\"]) * adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2066189806671399"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_r_squared(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which isn't hugely different. We have 434 observations and 6 features. Once you get into big data territory, the difference will diminish although \"big data\" is relative. If you have 10,000 observations but 1,000 features, your data isn't really bigger than having 1,000 observations but 10 features.\n",
    "\n",
    "Second, we note that the error of the regression ($\\sigma$) is 18.14 IQ points. What does that error really mean? Answering that question requires us to go on a bit of a tangent.\n",
    "\n",
    "### Estimating with Linear Regression\n",
    "\n",
    "The mean value of mom_iq and mom_age are, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.99999999999999"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_iq.mom_iq.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.785714285714285"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_iq.mom_age.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that's an IQ of 100 (which makes sense!) and an age of 23. If we consider the group of mothers with IQs of 100, who graduated high school, who was 23 years of age, and did not work during the first three years of the child's life we have a feature *vector* of [1, 100.0, 23, 0, 0, 0]. However, given the way the `models` module is set up, we need to include $x_0 = 1$ for the intercept *explicitly* so we have: [**1**, 1, 100.0, 23, 0, 0, 0].\n",
    "\n",
    "\n",
    "Using that feature vector in the `predict` function of the `LinearRegression` model, we have the following $\\hat{child\\_iq}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.96983127074502"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = result[\"model\"].predict(np.array([[1, 1, 100.0, 23.0, 0, 0, 0]]))[0][0]\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the double array in the function call to `predict` is because predict is set up to take more than one feature vector and return an *array* of predictions.\n",
    "\n",
    "The predicted *average* IQ of children whose mothers have those characteristics is about 86. And this is where $\\sigma$ or the error of the regression comes in. Remember that $\\sigma$ is a standard deviation. If these errors are indeed normally distributed, we expect the true value of the child's IQ to be within 1 standard deviation of the prediction 68% of the time; 2 standard deviations, 95% of the time, and 3 standard deviations, 99.7% of the time (the exact figures are 68.27%, 95.45%, and 99.73%...I don't know why we only show the decimal on the last one).\n",
    "\n",
    "We can do a quick estimate of the 95% bounds on our estimate is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49.69, 122.24)\n"
     ]
    }
   ],
   "source": [
    "print(\"({0:.2f}, {1:.2f})\".format(predicted - 2 * result[\"sigma\"], predicted + 2 * result[\"sigma\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that better than the \"null\" model or just using the mean child IQ. The mean and standard deviation of child IQ's are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child_iq = 86.80 (20.41)\n"
     ]
    }
   ],
   "source": [
    "mean_child_iq = child_iq.child_iq.mean()\n",
    "std_child_iq = child_iq.child_iq.std()\n",
    "print(\"child_iq = {0:.2f} ({1:.2f})\".format(mean_child_iq, std_child_iq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for any child, regardless of the characteristics of the mothers, we would predict an IQ of 86.8 and the 95% bounds would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45.98, 127.62)\n"
     ]
    }
   ],
   "source": [
    "print(\"({0:.2f}, {1:.2f})\".format(mean_child_iq - 2 * std_child_iq, mean_child_iq + 2 * std_child_iq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've gotten a *little* better?\n",
    "\n",
    "I know what you're saying...wait...confidence intervals? Well, let's punt on that for now. Given what we now know, it doesn't hurt to do some quick calculations assuming some mathematical distribution (Normal) and giving them a Bayesian interpretation. Let's see what happens if we do it right...\n",
    "\n",
    "In the result map, we have stored all the Bootstrap estimates for the coefficients and therefore enough information to do bootstrap estimates of the posterior distribution of a prediction. We can write a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_prediction(result, feature_values):\n",
    "    # resampled_coefficients is a DataFrame in result.\n",
    "    # each *row* is a different bootstrap model.\n",
    "    # we use a Dict instead of Vector because the order may not be preserved.\n",
    "    # we should modify this to use the model to pick the values and possibly\n",
    "    # convert interactions terms.\n",
    "    results = []\n",
    "    for coefficients in result[\"resampled_coefficients\"].itertuples():\n",
    "        estimate = 0\n",
    "        for feature in feature_values.keys():\n",
    "            estimate += feature_values[feature] * getattr(coefficients, feature)\n",
    "        results.append(estimate)\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of all of our predictions is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.21523981347428"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_values = {\"intercept\": 1, \"mom_hs\": 1, \"mom_iq\": 100, \"mom_age\": 23}\n",
    "posterior_prediction = bootstrap_prediction(result, feature_values)\n",
    "posterior_prediction.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(20, 6))\n",
    "\n",
    "axes = figure.add_subplot(1, 1, 1)\n",
    "axes.hist(posterior_prediction, color=\"DimGray\", density=True)\n",
    "axes.set_xlabel( \"Child IQ\")\n",
    "axes.set_ylabel( \"Density\")\n",
    "axes.set_title(\"Histogram of Child IQ Predictions for HS Graduated Mother with IQ 100, Aged 23\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, snap. The distribution is grossly normal--that may be an artifact of the number of bootstrap samples we took; we should consider increasing them for the final model. For now, our 95% credible interval (or Bayesian confidence interval) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% BCI [82.59661173 90.26274411]\n"
     ]
    }
   ],
   "source": [
    "print(\"95% BCI\", stats.mstats.mquantiles( posterior_prediction, [0.025, 0.975]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this bound is a lot tighter. Why? Because our feature values are at their means and $\\sigma$ is for the *entire* range of each feature. The model will be its most accurate at the mean values of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients\n",
    "\n",
    "Now that we can interpret the $R^2$ and $\\sigma$ of the regression, continue with the coefficients which we repeat below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <table>\n",
       "    <tr><th> model </th><td> child_iq ~ mom_hs + mom_iq + mom_age + mom_worked_2 + mom_worked_3 + mom_worked_4 </td></tr>\n",
       "    <tr><th colspan=2>coefficients</th><th>95% BCI</th></tr>\n",
       "    <tr><th> $\\beta_0$ </th><td> 20.27 </td><td>(-0.82, 39.32)</td></tr><tr><td>  mom_hs  ($\\beta_1$) </td><td> 5.43 </td><td>(0.83, 9.64)</tr><tr><td>  mom_iq  ($\\beta_2$) </td><td> 0.55 </td><td>(0.43, 0.67)</tr><tr><td>  mom_age  ($\\beta_3$) </td><td> 0.22 </td><td>(-0.57, 0.85)</tr><tr><td>  mom_worked_2  ($\\beta_4$) </td><td> 2.98 </td><td>(-2.23, 9.33)</tr><tr><td>  mom_worked_3  ($\\beta_5$) </td><td> 5.49 </td><td>(0.14, 10.18)</tr><tr><td>  mom_worked_4 ($\\beta_6$) </td><td> 1.42 </td><td>(-2.91, 5.66)</tr><tr><th colspan=2>metrics</th><th>95% BCI</th></tr><tr><th> $\\sigma$ </th><td> 18.14 </td><td>(17.06, 19.24) </td></tr><tr><th> $R^2$ </th><td> 0.22 </td><td>(0.15, 0.30)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.describe_bootstrap_lr(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our predictions before we ran the model? Let's write a quick function that compares our predictions based on domain knowledge and the actual values we estimate, in terms of probabilities. Note that this is better than asking if something is \"statistically significant\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"var1\": \"+\", \"var2\": \"-\"}\n",
    "def evaluate_coefficient_predictions(predictions, result):\n",
    "    coefficients = result[\"resampled_coefficients\"].columns\n",
    "    for coefficient in coefficients:\n",
    "        if coefficient == 'intercept':\n",
    "            continue\n",
    "        if predictions[coefficient] == '+':\n",
    "            print(\"{0} P(>0)={1:.3f}\".format(coefficient, np.mean(result[\"resampled_coefficients\"][coefficient] > 0)))\n",
    "        else:\n",
    "            print(\"{0} P(<0)={1:.3f}\".format(coefficient, np.mean(result[\"resampled_coefficients\"][coefficient] < 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mom_hs P(>0)=0.990\n",
      "mom_iq P(>0)=1.000\n",
      "mom_age P(>0)=0.780\n",
      "mom_worked_2 P(<0)=0.150\n",
      "mom_worked_3 P(<0)=0.010\n",
      "mom_worked_4 P(<0)=0.300\n"
     ]
    }
   ],
   "source": [
    "predictions = {\n",
    "    \"mom_hs\": '+',\n",
    "    \"mom_iq\": '+',\n",
    "    \"mom_age\": '+',\n",
    "    \"mom_worked_2\": '-',\n",
    "    \"mom_worked_3\": '-',\n",
    "    \"mom_worked_4\": '-'}\n",
    "evaluate_coefficient_predictions(predictions, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that mom_hs has a very high probability of being positive as does mom_iq. mom_age might be positive; there's a 78% chance, based on the data, that it is. However, for the other three variables that we thought would be negative, there's a very low probability that they are: 15%, 1%, and 30% respectively. What are we to do?\n",
    "\n",
    "## Guidelines for Evaluating Coefficients\n",
    "\n",
    "The general guidelines for evaluating coefficients when working with posterior distributions and credible intervals instead of confidence intervals changes a little bit. When we have a confidence interval of (-0.23, 5.97) we note that the coefficient *might* be zero and we just don't know. However, if we have a credible interval with the same bounds and the posterior distribution, we might say there's an 83% probability that the coefficient is positive (or a 17% probability that it's negative).\n",
    "\n",
    "| Case | Sign | Credible Interval | Action |\n",
    "|------|------------|--------------------|------------|\n",
    "| 1    | expected | does not include 0 | Keep  |\n",
    "| 2    | expected | does include 0 | Keep |\n",
    "| 3    | unexpected | does not include 0 | Re-examine |\n",
    "| 4    | unexpected | does include 0 | Remove |\n",
    "\n",
    "Basically, for this interpretation, we're treating a 95% *credible* interval that contains 0 as some evidence that the coefficient might be (however slim) the oppose sign. Alternatively, we might interpret ranges of the posterior distribution as [0, 33%) \"weak\" evidence, [33, 66%), \"mixed\" evidence, and [66%, 100%] \"strong\" evidence for a particular sign. *Don't get hung up on it*. Use your judgment and experience. If you aren't sure, go with the interval  interpretation until you get experience.\n",
    "\n",
    "So, why are these rules of thumb the way they are and why they might be reasonable?\n",
    "\n",
    "In all cases, we can think of \"the expected sign\" as an informal prior probability of at least the *direction* of influence by the variable, positive or negative. In actual Bayesian modeling it is possible to include this information as an actual prior probability on $\\beta_i$ but we're doing this informally here.\n",
    "\n",
    "**Case 1**\n",
    "\n",
    "So in the first case, say we have an informal prior of \"positive\", an actual positive coefficient and the confidence interval that does not include zero. This implies that the confidence interval includes only positive values. Since our informal prior was only about the *sign*, we should keep the variable.\n",
    "\n",
    "**Case 2**\n",
    "\n",
    "In the second case, say we have an informal prior of \"positive\", an actual positive coefficient but the confidence interval includes 0. Remember that the confidence interval is really a section of the posterior probability of the $\\beta_i$ in question. If the coefficient is positive and the confidence interval is symmetric, then there must be a greater than 50% probability that the coefficient is positive. This is why we keep it. But since we have the posterior probability, we might be able to find out if the probability really *is* 50/50 and adjust accordingly.\n",
    "\n",
    "**Case 3**\n",
    "\n",
    "In the third case, say we have an informal prior of \"positive\", an actual negative coefficient and a confidence interval that does not include zero. This means that we are pretty confident that, given the data, the probability of the value being negative is 95%. This means that we need to re-evaluate our beliefs about the variables.\n",
    "\n",
    "For example, if the variable is incumbency. In the US, this is often positively correlated with re-election but in India, incumbency is negatively correlated. The unexpected sign might also mean that some unknown variable is missing. Yes, this can happen, too.\n",
    "\n",
    "**Case 4**\n",
    "\n",
    "In the fourth case, say we have an informal prior of \"positive\", an actual negative coefficient and a confidence interval that includes zero. Because it includes zero and the coefficient is negative, there is a greater than 50% chance the coefficient is actually negative but the evidence is not overwhelming. We could also consider this under the third case if the confidence interval is really skewed towards negative when we expected positive. That is, we can bring in our ideas of \"weak\", \"mixed\" and \"strong\" evidence.\n",
    "\n",
    "Of course, the reasoning works if we make all the necessary substitutions starting with \"say we have an informal prior of 'negative'\". We can also think of this in terms of ROPE (region of practical significance), we think of the base line as being zero or not and things \"near zero\" might be practically zero.\n",
    "\n",
    "This method of evaluation means thinking ahead of time (remember *validity*?) about the expected sign of the coefficients. This emerges implicitly from our domain knowledge or explicitly through Causal Loop Diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Coefficients (Again)\n",
    "\n",
    "There's one last thing we haven't addressed about coefficients.\n",
    "\n",
    "We already talked about the predictive and the causal interpretations of coefficients. The predictive interpretation is not quite as easy to describe. Basically, linear models are predicting means of some kind. If we look at a set of specific values for a set of features then we're trying to predict the mean of the target variable for that set of feature values. Using child_iq again, if we think of mothers with IQs of 100, then we're predicting the mean IQ of their children. And the $beta_i$ of mom_iq, predicts the change in the mean when we look at mothers with IQs of 101.\n",
    "\n",
    "The causal interpretation is closer to the \"slope\" idea of a coefficient. If mother's IQ increases by 1 point, then we expect the child's IQ to change by $\\beta_i$, on average. This is also called the *counterfactual* interpretation.\n",
    "\n",
    "But there's a question that is often asked about linear models that we haven't yet addressed. When you have a model of 10 features, which coefficient, $\\beta_i$, is more important? or strongest? And it turns out that this is not an easy question to answer.\n",
    "\n",
    "The main reason is that the $\\beta_i$ coefficients have *two* jobs to do. First, they measure the *effect*. But, second, they *scale* one metric into another. While mom_iq points and child_iq points are in the same units, mom_age and child_iq are not. They are very nearly not even of the same magnitudes and consider a coefficient that must convert from bedrooms to prices in hundreds of thousands of dollars.\n",
    "\n",
    "Because of this, a large coefficient may be both converting a small number into a larger number's domain and representing a small effect while a smaller coefficient may be converting numbers into the same ranges but represents a (relatively) large effect. It is because of this that it's generally difficult to talk about which coefficients are more important.\n",
    "\n",
    "We will talk about ways of working around this when we talk about transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But suppose our model isn't very good. Can we make it better? Where should we look? We'll cover that in the remaining sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
