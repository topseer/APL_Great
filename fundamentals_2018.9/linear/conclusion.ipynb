{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this chapter we developed the basics of the two main linear models: linear regression and logistic regression. There is a whole family of linear models under the umbrella term *Generalized Linear Models* (GLM). For example, there is Poisson regression for estimating counts.\n",
    "\n",
    "For both models, we motivated the development of the linear model as an alternative to the constant model using the mean of the target variable. As we noted this baseline model is often called the \"null\" model and we often compare our development efforts against the null model.\n",
    "\n",
    "For linear regression, we examined models with numerical features and a binary categorical feature and learned to interpret them. We also introduced interaction terms. We applied Bayesian Bootstrap inference to our models and saw how they enable us to estimate credible intervals and probabilities for different parts of the model.\n",
    "\n",
    "For logistic regression, we concentrated on the difference in interpretation that logistic regression requires. In general, the same techniques that can be used for linear regression (numerical features, binary categorical features, interaction terms) can be used for logistic regression.\n",
    "\n",
    "In the next chapter, we'll discuss many more topics in linear and logistic regression such as model building, residuals, transformations, troubleshooting, and plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "1. How does a linear regression model of a numerical target $y$ extend the concept of the mean of $y$?\n",
    "2. If $x_1$ is a *categorical* feature with two outcomes, how do we interpret the $\\beta$s for the regression: $\\hat{y} = \\beta_0 + \\beta_1 x_1$?\n",
    "3. If $x_1$ is a *numerical* feature, how do we interpret the $\\beta$s for the regression: $\\hat{y} = \\beta_0 + \\beta_1 x_1$?\n",
    "4. If $x_1$ is a *numerical* feature and $x_2$ is a *categorical* feature of two outcomes, how do we interpret the $\\beta$s for the regression: $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2$?\n",
    "5. What is an *interaction term* and why might we include them in a linear regression?\n",
    "6. Describe the *predictive* and *causal* interpretations of the coefficients of a linear regression.\n",
    "7. Why can't we use linear regression for a binary categorical $y$?\n",
    "8. How is a logistic regression model of a binary categorical $y$ extend the concept of the mean of $y$?\n",
    "9. If $y$ takes on more than two values, what are our alternatives?\n",
    "10. How do we interpret $\\beta_0$ in logistic regression?\n",
    "11. How do we interpret any other $\\beta_i$ in logistic regression?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
